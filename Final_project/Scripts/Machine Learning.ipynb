{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the this part, we will try and implement a few machine learning algorithms over the original dataset without adding any external features or factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use RMSLE(Root Mean Square Logarithamic Error) as performance measure. \n",
    "RMSLE measures the ratio between actual and predicted.\n",
    "\n",
    "log(pi+1)−log(ai+1)\n",
    "\n",
    "can be written as log((pi+1)/(ai+1))\n",
    "\n",
    "It can be used when you don’t want to penalize huge differences when both the values are huge numbers.\n",
    "\n",
    "Also, this can be used when you want to penalize under estimates more than over estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>XGBoost</h2>\n",
    "\n",
    "XGBoost is an algorithm that has recently been dominating applied machine learning for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. \n",
    "Why Use XGBoost?\n",
    "The two reasons to use XGBoost are also the two goals of the project:\n",
    "\n",
    "Execution Speed.\n",
    "Model Performance.\n",
    "1. XGBoost Execution Speed\n",
    "Generally, XGBoost is fast. Really fast when compared to other implementations of gradient boosting.\n",
    "2. XGBoost Model Performance\n",
    "XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How to install xgboost module in Python</h3>\n",
    "\n",
    "1. Download the Appropriate .whl file for your environment from https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost\n",
    "for me as I am using 64-bit Python 3.6, I downloaded xgboost-0.71-cp36-cp36m-win_amd64.whl file. My python is 64-bit and not my system, so make sure you check that before downloading the .whl file\n",
    "\n",
    "2. Open your command prompt and cd into the downloaded folder.\n",
    "\n",
    "3. Now simply issue the pip install command to the downloaded .whl file like so:\n",
    "\n",
    "   <b>pip install xgboost-0.71-cp36-cp36m-win_amd64.whl</b>\n",
    "   \n",
    "4. If you have downloaded the wrong file it will give you an error saying whl file not supported for this platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from haversine import haversine\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import *\n",
    "from matplotlib import cm\n",
    "from matplotlib import animation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading and Loading external data into a dataframe</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninad\\Desktop\\BDSIN\\Final Project\\Scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "df_train = pd.read_csv(path+'/train.csv', parse_dates=['pickup_datetime','dropoff_datetime'])\n",
    "fast_route_1 = pd.read_csv(path+'/fastest_routes_train_part_1.csv')\n",
    "\n",
    "fast_route_2 = pd.read_csv(path+'/fastest_routes_train_part_2.csv')\n",
    "\n",
    "fast_route = pd.concat([fast_route_1,fast_route_2])\n",
    "fast_route_new = fast_route[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\n",
    "train_df = pd.merge(df_train, fast_route_new, on = 'id', how = 'left')\n",
    "train_new = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime    dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55 2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35 2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24 2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31 2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55 2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                  N            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading the test file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path+'/test.csv', parse_dates=['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting date from the datetime column from test and train\n",
    "train_df.loc[:, 'pickup_date'] = train_df['pickup_datetime'].dt.date\n",
    "train_df.loc[:, 'dropoff_date'] = train_df['dropoff_datetime'].dt.date\n",
    "test_df.loc[:, 'pickup_date'] = test_df['pickup_datetime'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the store_and_fwd_flag into categorical variable\n",
    "train_df['store_and_fwd_flag'] = 1 * (train_df.store_and_fwd_flag.values == 'Y')\n",
    "test_df['store_and_fwd_flag'] = 1 * (test_df.store_and_fwd_flag.values == 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>total_distance</th>\n",
       "      <th>total_travel_time</th>\n",
       "      <th>number_of_steps</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>dropoff_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>0</td>\n",
       "      <td>455</td>\n",
       "      <td>2009.1</td>\n",
       "      <td>164.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>2016-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>0</td>\n",
       "      <td>663</td>\n",
       "      <td>2513.2</td>\n",
       "      <td>332.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>2016-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>0</td>\n",
       "      <td>2124</td>\n",
       "      <td>11060.8</td>\n",
       "      <td>767.6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>2016-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>0</td>\n",
       "      <td>429</td>\n",
       "      <td>1779.4</td>\n",
       "      <td>235.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>2016-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "      <td>1614.9</td>\n",
       "      <td>140.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>2016-03-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime    dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55 2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35 2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24 2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31 2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55 2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude  store_and_fwd_flag  trip_duration  total_distance  \\\n",
       "0         40.765602                   0            455          2009.1   \n",
       "1         40.731152                   0            663          2513.2   \n",
       "2         40.710087                   0           2124         11060.8   \n",
       "3         40.706718                   0            429          1779.4   \n",
       "4         40.782520                   0            435          1614.9   \n",
       "\n",
       "   total_travel_time  number_of_steps pickup_date dropoff_date  \n",
       "0              164.9              5.0  2016-03-14   2016-03-14  \n",
       "1              332.0              6.0  2016-06-12   2016-06-12  \n",
       "2              767.6             16.0  2016-01-19   2016-01-19  \n",
       "3              235.8              4.0  2016-04-06   2016-04-06  \n",
       "4              140.1              5.0  2016-03-26   2016-03-26  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Feature Extraction</b></h3>\n",
    "\n",
    "In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.\n",
    "\n",
    "When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection.The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data\n",
    "\n",
    "<h3>Principal Component Analysis</h3>\n",
    "\n",
    "Sometimes data are collected on a large number of variables from a single population. With a large number of variables, the dispersion matrix may be too large to study and interpret properly. There would be too many pairwise correlations between the variables to consider. Graphical displays may also not be particularly helpful when the data set is very large. With 12 variables, for example, there will be more than 200 three-dimensional scatterplots. To interpret the data in a more meaningful form, it is necessary to reduce the number of variables to a few, interpretable linear combinations of the data. Each linear combination will correspond to a principal component.<b>We use PCA to transform longitude and latitude coordinates.\n",
    "In this case it is not about dimension reduction since we transform 2D-> 2D. The rotation could help for decision tree splits.</b>. With rotation the principal components are more specifically associated with the original variables. The maximum likelihood solution is undefined down to an orthogonal transformation (ie rotations) of the factor loadings. That is, any rotation of the factor loadings is an equally good solutions. So it makes sense to rotate these loadings in a way which maximises ease of interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack arrays in sequence vertically (row wise).\n",
    "coords = np.vstack((train_df[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    train_df[['dropoff_latitude', 'dropoff_longitude']].values,\n",
    "                    train_df[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    train_df[['dropoff_latitude', 'dropoff_longitude']].values))\n",
    "\n",
    "pca = PCA().fit(coords)\n",
    "\n",
    "train_df['pickup_pca0'] = pca.transform(train_df[['pickup_latitude', 'pickup_longitude']])[:,0]\n",
    "train_df['pickup_pca1'] = pca.transform(train_df[['pickup_latitude', 'pickup_longitude']])[:,1]\n",
    "train_df['dropoff_pca0'] = pca.transform(train_df[['dropoff_latitude', 'dropoff_longitude']])[:,0]\n",
    "train_df['dropoff_pca1'] = pca.transform(train_df[['dropoff_latitude', 'dropoff_longitude']])[:,1]\n",
    "\n",
    "test_df['pickup_pca0'] = pca.transform(test_df[['pickup_latitude', 'pickup_longitude']])[:,0]\n",
    "test_df['pickup_pca1'] = pca.transform(test_df[['pickup_latitude', 'pickup_longitude']])[:,1]\n",
    "test_df['dropoff_pca0'] = pca.transform(test_df[['dropoff_latitude', 'dropoff_longitude']])[:,0]\n",
    "test_df['dropoff_pca1'] = pca.transform(test_df[['dropoff_latitude', 'dropoff_longitude']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>total_distance</th>\n",
       "      <th>total_travel_time</th>\n",
       "      <th>number_of_steps</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>dropoff_date</th>\n",
       "      <th>pickup_pca0</th>\n",
       "      <th>pickup_pca1</th>\n",
       "      <th>dropoff_pca0</th>\n",
       "      <th>dropoff_pca1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>0</td>\n",
       "      <td>455</td>\n",
       "      <td>2009.1</td>\n",
       "      <td>164.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>-0.009496</td>\n",
       "      <td>0.013801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>0</td>\n",
       "      <td>663</td>\n",
       "      <td>2513.2</td>\n",
       "      <td>332.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>-0.012447</td>\n",
       "      <td>0.026972</td>\n",
       "      <td>-0.018933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>0</td>\n",
       "      <td>2124</td>\n",
       "      <td>11060.8</td>\n",
       "      <td>767.6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.033831</td>\n",
       "      <td>-0.039692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>0</td>\n",
       "      <td>429</td>\n",
       "      <td>1779.4</td>\n",
       "      <td>235.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>0.038057</td>\n",
       "      <td>-0.029593</td>\n",
       "      <td>0.040920</td>\n",
       "      <td>-0.042722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "      <td>1614.9</td>\n",
       "      <td>140.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>-0.002411</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>-0.002026</td>\n",
       "      <td>0.031099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime    dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55 2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35 2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24 2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31 2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55 2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude  store_and_fwd_flag  trip_duration  total_distance  \\\n",
       "0         40.765602                   0            455          2009.1   \n",
       "1         40.731152                   0            663          2513.2   \n",
       "2         40.710087                   0           2124         11060.8   \n",
       "3         40.706718                   0            429          1779.4   \n",
       "4         40.782520                   0            435          1614.9   \n",
       "\n",
       "   total_travel_time  number_of_steps pickup_date dropoff_date  pickup_pca0  \\\n",
       "0              164.9              5.0  2016-03-14   2016-03-14     0.007896   \n",
       "1              332.0              6.0  2016-06-12   2016-06-12     0.007572   \n",
       "2              767.6             16.0  2016-01-19   2016-01-19     0.004964   \n",
       "3              235.8              4.0  2016-04-06   2016-04-06     0.038057   \n",
       "4              140.1              5.0  2016-03-26   2016-03-26    -0.002411   \n",
       "\n",
       "   pickup_pca1  dropoff_pca0  dropoff_pca1  \n",
       "0     0.016976     -0.009496      0.013801  \n",
       "1    -0.012447      0.026972     -0.018933  \n",
       "2     0.012832      0.033831     -0.039692  \n",
       "3    -0.029593      0.040920     -0.042722  \n",
       "4     0.041781     -0.002026      0.031099  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Haversine Distance:</b> The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes.\n",
    "\n",
    "<b>Manhattan Distance:</b> The distance between two points in a grid based on a strictly horizontal and/or vertical path (that is, along the grid lines), as opposed to the diagonal or \"as the crow flies\" distance.\n",
    "\n",
    "<b>Bearing Distance:</b> The bearing (or azimuth) is the compass direction to travel from the starting point, and must be within the range 0 to 360. 0 represents north, 90 is east, 180 is south and 270 is west."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) #applies a function to all the items in an input_list\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    a = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) #applies a function to all the items in an input_list\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the above three functions to the columns in train and test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'distance_haversine'] = haversine_array(train_df['pickup_latitude'].values, train_df['pickup_longitude'].values, train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "train_df.loc[:, 'distance_manhattan'] = dummy_manhattan_distance(train_df['pickup_latitude'].values, train_df['pickup_longitude'].values, train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "train_df.loc[:, 'direction'] = bearing_array(train_df['pickup_latitude'].values, train_df['pickup_longitude'].values, train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "train_df.loc[:, 'pca_manhattan'] = np.abs(train_df['dropoff_pca1'] - train_df['pickup_pca1']) + np.abs(train_df['dropoff_pca0'] - train_df['pickup_pca0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[:, 'distance_haversine'] = haversine_array(test_df['pickup_latitude'].values, test_df['pickup_longitude'].values, test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "test_df.loc[:, 'distance_manhattan'] = dummy_manhattan_distance(test_df['pickup_latitude'].values, test_df['pickup_longitude'].values, test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "test_df.loc[:, 'direction'] = bearing_array(test_df['pickup_latitude'].values, test_df['pickup_longitude'].values, test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "test_df.loc[:, 'pca_manhattan'] = np.abs(test_df['dropoff_pca1'] - test_df['pickup_pca1']) + np.abs(test_df['dropoff_pca0'] - test_df['pickup_pca0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'center_latitude'] = (train_df['pickup_latitude'].values + train_df['dropoff_latitude'].values)/2\n",
    "train_df.loc[:, 'center_longitude'] = (train_df['pickup_longitude'].values + train_df['dropoff_longitude'].values)/2\n",
    "test_df.loc[:, 'center_latitude'] = (test_df['pickup_latitude'].values + test_df['dropoff_latitude'].values)/2\n",
    "test_df.loc[:, 'center_longitude'] = (test_df['pickup_longitude'].values + test_df['dropoff_longitude'].values)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Extract all the datetime features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'pickup_weekday'] = train_df['pickup_datetime'].dt.weekday\n",
    "train_df.loc[:, 'pickup_hour_weekofyear'] = train_df['pickup_datetime'].dt.weekofyear\n",
    "train_df.loc[:, 'pickup_hour'] = train_df['pickup_datetime'].dt.hour\n",
    "train_df.loc[:, 'pickup_minute'] = train_df['pickup_datetime'].dt.minute\n",
    "train_df.loc[:, 'pickup_dt'] = (train_df['pickup_datetime'] - train_df['pickup_datetime'].min()).dt.total_seconds()\n",
    "train_df.loc[:, 'pickup_week_hour'] = train_df['pickup_weekday'] * 24 + train_df['pickup_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[:, 'pickup_weekday'] = test_df['pickup_datetime'].dt.weekday\n",
    "test_df.loc[:, 'pickup_hour_weekofyear'] = test_df['pickup_datetime'].dt.weekofyear\n",
    "test_df.loc[:, 'pickup_hour'] = test_df['pickup_datetime'].dt.hour\n",
    "test_df.loc[:, 'pickup_minute'] = test_df['pickup_datetime'].dt.minute\n",
    "test_df.loc[:, 'pickup_dt'] = (test_df['pickup_datetime'] - test_df['pickup_datetime'].min()).dt.total_seconds()\n",
    "test_df.loc[:, 'pickup_week_hour'] = test_df['pickup_weekday'] * 24 + test_df['pickup_hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract the Speed feature using the Haversine Distance and trip duration</h3>\n",
    "\n",
    "We will use the basic speed is equal to distance upon time formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'avg_speed_h'] = 1000 * train_df['distance_haversine']/train_df['trip_duration']\n",
    "train_df.loc[:, 'avg_speed_m'] = 1000 * train_df['distance_manhattan']/train_df['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>center_latitude</th>\n",
       "      <th>center_longitude</th>\n",
       "      <th>pickup_weekday</th>\n",
       "      <th>pickup_hour_weekofyear</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_minute</th>\n",
       "      <th>pickup_dt</th>\n",
       "      <th>pickup_week_hour</th>\n",
       "      <th>avg_speed_h</th>\n",
       "      <th>avg_speed_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.766769</td>\n",
       "      <td>-73.973392</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>6369878.0</td>\n",
       "      <td>17</td>\n",
       "      <td>3.293452</td>\n",
       "      <td>3.814139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.734858</td>\n",
       "      <td>-73.989948</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>14085798.0</td>\n",
       "      <td>144</td>\n",
       "      <td>2.723239</td>\n",
       "      <td>3.665922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.737013</td>\n",
       "      <td>-73.992180</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>1596907.0</td>\n",
       "      <td>35</td>\n",
       "      <td>3.006167</td>\n",
       "      <td>3.862323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.713345</td>\n",
       "      <td>-74.011154</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>8364734.0</td>\n",
       "      <td>67</td>\n",
       "      <td>3.462700</td>\n",
       "      <td>3.872567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.787865</td>\n",
       "      <td>-73.972988</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>7392638.0</td>\n",
       "      <td>133</td>\n",
       "      <td>2.732387</td>\n",
       "      <td>2.757372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime    dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55 2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35 2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24 2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31 2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55 2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude  store_and_fwd_flag     ...       center_latitude  \\\n",
       "0         40.765602                   0     ...             40.766769   \n",
       "1         40.731152                   0     ...             40.734858   \n",
       "2         40.710087                   0     ...             40.737013   \n",
       "3         40.706718                   0     ...             40.713345   \n",
       "4         40.782520                   0     ...             40.787865   \n",
       "\n",
       "   center_longitude  pickup_weekday  pickup_hour_weekofyear pickup_hour  \\\n",
       "0        -73.973392               0                      11          17   \n",
       "1        -73.989948               6                      23           0   \n",
       "2        -73.992180               1                       3          11   \n",
       "3        -74.011154               2                      14          19   \n",
       "4        -73.972988               5                      12          13   \n",
       "\n",
       "  pickup_minute   pickup_dt  pickup_week_hour  avg_speed_h  avg_speed_m  \n",
       "0            24   6369878.0                17     3.293452     3.814139  \n",
       "1            43  14085798.0               144     2.723239     3.665922  \n",
       "2            35   1596907.0                35     3.006167     3.862323  \n",
       "3            32   8364734.0                67     3.462700     3.872567  \n",
       "4            30   7392638.0               133     2.732387     2.757372  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the names of all the columns in train into a list and the list of not required columns to exclude list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendor_id                 1458644\n",
       "passenger_count           1458644\n",
       "pickup_longitude          1458644\n",
       "pickup_latitude           1458644\n",
       "dropoff_longitude         1458644\n",
       "dropoff_latitude          1458644\n",
       "store_and_fwd_flag        1458644\n",
       "pickup_pca0               1458644\n",
       "pickup_pca1               1458644\n",
       "dropoff_pca0              1458644\n",
       "dropoff_pca1              1458644\n",
       "distance_haversine        1458644\n",
       "distance_manhattan        1458644\n",
       "direction                 1458644\n",
       "pca_manhattan             1458644\n",
       "center_latitude           1458644\n",
       "center_longitude          1458644\n",
       "pickup_weekday            1458644\n",
       "pickup_hour_weekofyear    1458644\n",
       "pickup_hour               1458644\n",
       "pickup_minute             1458644\n",
       "pickup_dt                 1458644\n",
       "pickup_week_hour          1458644\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = list(train_df.columns) #list of all the column names \n",
    "exclude = ['id','pickup_datetime','dropoff_datetime','trip_duration','pickup_date','dropoff_date','avg_speed_h','avg_speed_m','total_distance','total_travel_time','number_of_steps']\n",
    "feature_names = [f for f in train_df.columns if f not in exclude] #will remove all the unnecessary features from the train set\n",
    "train_df[feature_names].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vendor_id', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag', 'pickup_pca0', 'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1', 'distance_haversine', 'distance_manhattan', 'direction', 'pca_manhattan', 'center_latitude', 'center_longitude', 'pickup_weekday', 'pickup_hour_weekofyear', 'pickup_hour', 'pickup_minute', 'pickup_dt', 'pickup_week_hour']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't want any datatypes other than int and float to pass into a machine learning algorithm we will first make sure of the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendor_id                   int64\n",
       "passenger_count             int64\n",
       "pickup_longitude          float64\n",
       "pickup_latitude           float64\n",
       "dropoff_longitude         float64\n",
       "dropoff_latitude          float64\n",
       "store_and_fwd_flag          int32\n",
       "pickup_pca0               float64\n",
       "pickup_pca1               float64\n",
       "dropoff_pca0              float64\n",
       "dropoff_pca1              float64\n",
       "distance_haversine        float64\n",
       "distance_manhattan        float64\n",
       "direction                 float64\n",
       "pca_manhattan             float64\n",
       "center_latitude           float64\n",
       "center_longitude          float64\n",
       "pickup_weekday              int64\n",
       "pickup_hour_weekofyear      int64\n",
       "pickup_hour                 int64\n",
       "pickup_minute               int64\n",
       "pickup_dt                 float64\n",
       "pickup_week_hour            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[feature_names].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************************************\n",
    "#*    This was adapted from a post from\n",
    "#*    Author: Kulbear\n",
    "#*    Date: 04/23/2018\n",
    "#*    Availability: https://github.com/Kulbear/New-York-City-Taxi-Trip-Duration/blob/master/xgboost.ipynb\n",
    "#*\n",
    "#***************************************************************************************/\n",
    "\n",
    "import xgboost as xgb\n",
    "y = np.log(train_df['trip_duration'].values + 1) #We will convert the trip duration into logarithmic values for the sake of ML\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_df[feature_names].values, y, test_size=0.2) #we will split the dataset into train and test\n",
    "\n",
    "#The data is stored in a DMatrix object\n",
    "#Missing values can be replaced by a default value in the DMatrix constructor\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(val_x, label=val_y )\n",
    "dtest = xgb.DMatrix(test_df[feature_names].values)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "# We have used random search for XGBoost\n",
    "xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n",
    "            'subsample': 0.8, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'eval_metric': 'rmse', 'objective': 'reg:linear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:4.2399\tvalid-rmse:4.23942\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 50 rounds.\n",
      "[10]\ttrain-rmse:0.435423\tvalid-rmse:0.442909\n",
      "[20]\ttrain-rmse:0.391352\tvalid-rmse:0.403887\n",
      "[30]\ttrain-rmse:0.380944\tvalid-rmse:0.397034\n",
      "[40]\ttrain-rmse:0.375141\tvalid-rmse:0.393524\n",
      "[50]\ttrain-rmse:0.371484\tvalid-rmse:0.391909\n",
      "[60]\ttrain-rmse:0.367723\tvalid-rmse:0.390297\n",
      "[70]\ttrain-rmse:0.366075\tvalid-rmse:0.390051\n",
      "[80]\ttrain-rmse:0.364056\tvalid-rmse:0.389532\n",
      "[90]\ttrain-rmse:0.362772\tvalid-rmse:0.38954\n",
      "[99]\ttrain-rmse:0.361529\tvalid-rmse:0.389679\n"
     ]
    }
   ],
   "source": [
    "# You could try to train with more epoch,the rmse value might stall after few epochs sometimes\n",
    "model = xgb.train(xgb_pars, dtrain, 100, watchlist, early_stopping_rounds=50,\n",
    "                  maximize=False, verbose_eval=10)\n",
    "\n",
    "#If you have a validation set, you can use early stopping to find the optimal number of boosting rounds. \n",
    "#Early stopping requires at least one set in evals. If there’s more than one, it will use the last.\n",
    "#train(..., evals=evals, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling RMSLE 0.38945\n"
     ]
    }
   ],
   "source": [
    "print('Modeling RMSLE %.5f' % model.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Regressor</h2>\n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.Random decision forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)\n",
    "\n",
    "Randomized search on hyper parameters.\n",
    "\n",
    "RandomizedSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.\n",
    "\n",
    "In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\n",
    "\n",
    "If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7441684346761585\n",
      "10\n",
      "30\n",
      "5\n",
      "10\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#***************************************************************************************\n",
    "#*    This was adapted from a post from\n",
    "#*    Author: Kulbear\n",
    "#*    Date: 04/23/2018\n",
    "#*    Availability: https://github.com/Kulbear/New-York-City-Taxi-Trip-Duration/blob/master/xgboost.ipynb\n",
    "#*\n",
    "#***************************************************************************************/\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf = RandomForestRegressor(random_state =42) #random_state is the seed used by the random number generator;\n",
    "\n",
    "#We will try and select the best parameters for Random Forest to give good accuracy\n",
    "param_grid = {\"max_depth\": [10,20,30],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11)}\n",
    "\n",
    "validator = RandomizedSearchCV(clf, param_distributions= param_grid) #clf is the estimator and\n",
    "#param_distributions : dict\n",
    "#Dictionary with parameters names (string) as keys and distributions or lists of parameters to try\n",
    "\n",
    "validator.fit(train_x,train_y)\n",
    "print(validator.best_score_)\n",
    "print(validator.best_estimator_.n_estimators)\n",
    "print(validator.best_estimator_.max_depth)\n",
    "print(validator.best_estimator_.min_samples_split)\n",
    "print(validator.best_estimator_.min_samples_leaf)\n",
    "print(validator.best_estimator_.max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>RandomForestRegressor</h6>\n",
    "\n",
    "<h6>max_depth</h6> : integer or None, optional (default=None)\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "<h6>min_samples_split</h6>int, float, optional (default=2)\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "<h6>min_samples_leaf</h6>int, float, optional (default=1)\n",
    "The minimum number of samples required to be at a leaf node:\n",
    "\n",
    "<h6>n_estimators</h6>: The number of trees in the forest.\n",
    "\n",
    "<h6>max_features</h6>: The number of features to consider when looking for the best split\n",
    "\n",
    "You can vary the features and see it the rmse value decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=5,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(max_depth = 20 , min_samples_split= 2, \n",
    "                                 min_samples_leaf=5, n_estimators =10, max_features =8)\n",
    "rf_model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on validation set\n",
    "predictions = rf_model.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4022884447324068\n"
     ]
    }
   ],
   "source": [
    "#RMSLE error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#tree_pred = pd.DataFrame(predictions)\n",
    "dec_mse = mean_squared_error(predictions, val_y)\n",
    "rmse = np.sqrt(dec_mse)\n",
    "print(rmse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lasso & Ridge Regularization</h2>\n",
    "\n",
    "Ridge and Lasso regression are powerful techniques generally used for creating parsimonious models in presence of a ‘large’ number of features. Here ‘large’ can typically mean either of two things:\n",
    "    1.Large enough to enhance the tendency of a model to overfit (as low as 10 variables might cause overfitting)\n",
    "    2.Large enough to cause computational challenges. With modern systems, this situation might arise in case of millions or   billions of features\n",
    "  \n",
    "Though Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. If you’ve heard of them before, you must know that they work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques. The key difference is in how they assign penalty to the coefficients:\n",
    "\n",
    "Ridge Regression:\n",
    "Performs L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
    "\n",
    "Lasso Regression:\n",
    "Performs L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of absolute value of coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>param_grid :</h6> dict of string to sequence, or sequence of such\n",
    "\n",
    "The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.\n",
    "\n",
    "An empty dict signifies default parameters.\n",
    "\n",
    "A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ridge</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28518283579068787\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "##Ridge\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#This model solves a regression model \n",
    "#where the loss function is the linear least squares function and regularization is given by the l2-norm\n",
    "ridge = Ridge()\n",
    "\n",
    "#param_grid will be give a range of values for alpha, which provides regularization strength in Ridge function\n",
    "param_grid = {\"alpha\": [0.01,0.05,0.001,0.0001,0.000001,0.005,0.0005,0.00005]}\n",
    "\n",
    "validator = GridSearchCV(ridge, param_grid= param_grid) \n",
    "validator.fit(train_x,train_y)\n",
    "print(validator.best_score_)\n",
    "print(validator.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='cholesky', tol=0.001)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.\n",
    "ridge_reg = Ridge(alpha = 0.05, solver = 'cholesky')\n",
    "ridge_reg.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge = ridge_reg.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6357417553779042\n"
     ]
    }
   ],
   "source": [
    "dec_mse_ridge = mean_squared_error(predictions_ridge, val_y)\n",
    "rmse_ridge = np.sqrt(dec_mse_ridge)\n",
    "print(rmse_ridge) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lasso</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3161891761105055\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "param_grid = {\"alpha\": [0.01,0.05,0.001,0.0001,0.000001,0.005,0.0005,0.00005]}\n",
    "\n",
    "validator = GridSearchCV(lasso, param_grid= param_grid) \n",
    "validator.fit(train_x,train_y)\n",
    "print(validator.best_score_)\n",
    "print(validator.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=1,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alpha: Constant that multiplies the L1 term\n",
    "#random_state: The seed of the pseudo random number generator that selects a random feature to update\n",
    "#The parameters can be changed\n",
    "lasso_reg=Lasso(alpha =0.05, random_state=1)\n",
    "lasso_reg.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lasso = lasso_reg.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6377002641499087\n"
     ]
    }
   ],
   "source": [
    "dec_mse_lasso = mean_squared_error(predictions_lasso,val_y)\n",
    "rmse_lasso = np.sqrt(dec_mse_lasso)\n",
    "print(rmse_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ensemble Learning</h2>\n",
    "\n",
    "An ensemble contains a number of learners which are usually called base learners. The generalization ability of an ensemble\n",
    "is usually much stronger than that of base learners. Actually, ensemble learning is appealing because that it is able to boost\n",
    "weak learners which are slightly better than random guess to strong learners which can make very accurate predictions. So,\n",
    "“base learners” are also referred as “weak learners”. It is noteworthy, however, that although most theoretical analyses work\n",
    "on weak learners, base learners used in practice are not necessarily weak since using not-so-weak base learners often results\n",
    "in better performance.\n",
    "\n",
    "Base learners are usually generated from training data by a base learning algorithm which can be decision tree, neural\n",
    "network or other kinds of machine learning algorithms. Most ensemble methods use a single base learning algorithm to\n",
    "produce homogeneous base learners, but there are also some methods which use multiple learning algorithms to produce\n",
    "heterogeneous learners. In the latter case there is no single base learning algorithm and thus, some people prefer calling the\n",
    "learners individual learners or component learners to “base learners”, while the names “individual learners” and “component\n",
    "learners” can also be used for homogeneous base learners.\n",
    "\n",
    "To see if we get better performance we will combine Random Forest, Lasso, Ridge and XGBRegressor and check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>XGBRegressor</h6>\n",
    "\n",
    "objective : string or callable\n",
    "Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below)\n",
    "\n",
    "n_estimators : int\n",
    "Number of boosted trees to fit\n",
    "\n",
    "subsample : float\n",
    "Subsample ratio of the training instance.\n",
    "\n",
    "max_depth : int\n",
    "Maximum tree depth for base learners\n",
    "\n",
    "min_child_weight : int\n",
    "Minimum sum of instance weight(hessian) needed in a child.\n",
    "\n",
    "reg_alpha : float (xgb’s alpha)\n",
    "L1 regularization term on weights\n",
    "reg_lambda : float (xgb’s lambda)\n",
    "L2 regularization term on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor(objective='reg:linear', n_estimators=10, subsample=0.9, max_depth = 6, min_child_weight = 3\n",
    "                        ,reg_lambda = 2, learning_rate = 0.05)\n",
    "\n",
    "rf_model = RandomForestRegressor(max_depth = 20 , min_samples_split= 2, min_samples_leaf=5, n_estimators =10, max_features =8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "\n",
    "\n",
    "class AveragingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, regressors):\n",
    "        self.regressors = regressors\n",
    "        self.predictions = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for regr in self.regressors:\n",
    "            regr.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.predictions = np.column_stack([regr.predict(X) for regr in self.regressors])\n",
    "        return np.mean(self.predictions, axis=1)\n",
    "    \n",
    "    \n",
    "averaged_model = AveragingRegressor([xgb_model, rf_model,lasso_reg,ridge_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_model.fit(train_x,train_y)\n",
    "predicitons_ensemble = averaged_model.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.039269769322409\n"
     ]
    }
   ],
   "source": [
    "dec_mse_ensemble = mean_squared_error(predicitons_ensemble, val_y)\n",
    "rmse_ensemble = np.sqrt(dec_mse_ensemble)\n",
    "print(rmse_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>\n",
    "\n",
    "An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.\n",
    "\n",
    "I have used keras module instead of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log)))\n",
    "\n",
    "rmsle = root_mean_squared_logarithmic_error\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = train_x.shape[-1]\n",
    "_input = Input(shape=(num_features,))\n",
    "#_norm = BatchNormalization()(_input)\n",
    "layer1 = Dense(150, activation='linear')(_input)\n",
    "layer1 = Dropout(0.2)(layer1)\n",
    "layer2 = Dense(20, activation='relu')(layer1)\n",
    "_output = Dense(1, activation='relu')(layer2)\n",
    "\n",
    "model = Model(inputs=[_input], outputs=[_output])\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 933532 samples, validate on 233383 samples\n",
      "Epoch 1/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0101 - val_loss: 2.0073\n",
      "Epoch 2/50\n",
      "933532/933532 [==============================] - 12s 12us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 3/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 4/50\n",
      "933532/933532 [==============================] - 11s 12us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 5/50\n",
      "933532/933532 [==============================] - 11s 12us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 6/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 7/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 8/50\n",
      "933532/933532 [==============================] - 11s 12us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 9/50\n",
      "933532/933532 [==============================] - 14s 15us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 10/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 11/50\n",
      "933532/933532 [==============================] - 11s 12us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 12/50\n",
      "933532/933532 [==============================] - 11s 12us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 13/50\n",
      "933532/933532 [==============================] - 18s 19us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 14/50\n",
      "933532/933532 [==============================] - 18s 19us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 15/50\n",
      "933532/933532 [==============================] - 16s 17us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 16/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 17/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 18/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 19/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 20/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 21/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 22/50\n",
      "933532/933532 [==============================] - 15s 17us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 23/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 24/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 25/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 26/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 27/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 28/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 29/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 30/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 31/50\n",
      "933532/933532 [==============================] - 16s 17us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 32/50\n",
      "933532/933532 [==============================] - 15s 17us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 33/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 34/50\n",
      "933532/933532 [==============================] - 16s 17us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 35/50\n",
      "933532/933532 [==============================] - 16s 17us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 36/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 37/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 38/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 39/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 40/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 41/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 42/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 43/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 44/50\n",
      "933532/933532 [==============================] - 15s 16us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 45/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 46/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 47/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 48/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 49/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n",
      "Epoch 50/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0073\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights1.h5', save_best_only=True,\n",
    "                             monitor='val_loss', verbose=0)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "              patience=10, verbose=1)\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 40:\n",
    "        return learning_rate\n",
    "    if epoch > 40 and epoch < 80:\n",
    "        return learning_rate * 0.1\n",
    "    else:\n",
    "        return learning_rate * 0.05\n",
    "    \n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "results = model.fit(train_x, train_y, \n",
    "                    batch_size=256,\n",
    "                    epochs=50, callbacks=[checkpoint],\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 42.44132037611975\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model.predict(val_x)\n",
    "print('Test MSE: {}'.format(mean_squared_error(val_y, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.007466\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "for yt, yth in zip(val_y, y_test_hat):\n",
    "    diff = np.log(yt + 1.) - np.log(yth + 1.)\n",
    "    test_loss.append(diff ** 2)\n",
    "print(np.sqrt(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "\n",
    "Evaluation of models\n",
    "XGBoost: 0.3894\n",
    "RandomForest: 0.4022\n",
    "Ridge: 0.6357\n",
    "Lasso: 0.6377\n",
    "EnsembleLearning: 1.039\n",
    "NeraulNetwork: 2.0074\n",
    "\n",
    "Among all the models XGBoost performed exceptionally well when compared to other algorithms\n",
    "It is not a rule that Ensemble Learning would give better results, sometimes it may perform worse than independent models\n",
    "\n",
    "More data can be added to the original dataset such as holidays and weather to get better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>License (MIT)</h1>\n",
    "\n",
    "Copyright (c) 2018 by Ninad Gadre\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
