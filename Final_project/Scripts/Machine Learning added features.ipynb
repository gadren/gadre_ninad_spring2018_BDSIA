{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the this part, we will try and implement a few machine learning algorithms over the original dataset with added external features or factors. We will add holiday and weather as features to help improve the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use RMSLE(Root Mean Square Logarithamic Error) as performance measure. \n",
    "RMSLE measures the ratio between actual and predicted.\n",
    "\n",
    "log(pi+1)−log(ai+1)\n",
    "\n",
    "can be written as log((pi+1)/(ai+1))\n",
    "\n",
    "It can be used when you don’t want to penalize huge differences when both the values are huge numbers.\n",
    "\n",
    "Also, this can be used when you want to penalize under estimates more than over estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>XGBoost</h2>\n",
    "\n",
    "XGBoost is an algorithm that has recently been dominating applied machine learning for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. \n",
    "Why Use XGBoost?\n",
    "The two reasons to use XGBoost are also the two goals of the project:\n",
    "\n",
    "Execution Speed.\n",
    "Model Performance.\n",
    "1. XGBoost Execution Speed\n",
    "Generally, XGBoost is fast. Really fast when compared to other implementations of gradient boosting.\n",
    "2. XGBoost Model Performance\n",
    "XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How to install xgboost module in Python</h3>\n",
    "\n",
    "1. Download the Appropriate .whl file for your environment from https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost\n",
    "for me as I am using 64-bit Python 3.6, I downloaded xgboost-0.71-cp36-cp36m-win_amd64.whl file. My python is 64-bit and not my system, so make sure you check that before downloading the .whl file\n",
    "\n",
    "2. Open your command prompt and cd into the downloaded folder.\n",
    "\n",
    "3. Now simply issue the pip install command to the downloaded .whl file like so:\n",
    "\n",
    "   <b>pip install xgboost-0.71-cp36-cp36m-win_amd64.whl</b>\n",
    "   \n",
    "4. If you have downloaded the wrong file it will give you an error saying whl file not supported for this platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from haversine import haversine\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import *\n",
    "from matplotlib import cm\n",
    "from matplotlib import animation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading and Loading external data into a dataframe</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninad\\Desktop\\BDSIN\\Final Project\\Scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path+'/train.csv', parse_dates=['pickup_datetime','dropoff_datetime'])\n",
    "fast_route_1 = pd.read_csv(path+'/fastest_routes_train_part_1.csv')\n",
    "\n",
    "fast_route_2 = pd.read_csv(path+'/fastest_routes_train_part_2.csv')\n",
    "\n",
    "fast_route = pd.concat([fast_route_1,fast_route_2])\n",
    "fast_route_new = fast_route[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\n",
    "train_df = pd.merge(df_train, fast_route_new, on = 'id', how = 'left')\n",
    "train_new = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime    dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55 2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35 2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24 2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31 2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55 2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                  N            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading the test file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>total_distance</th>\n",
       "      <th>total_travel_time</th>\n",
       "      <th>number_of_steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id3004672</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.988129</td>\n",
       "      <td>40.732029</td>\n",
       "      <td>-73.990173</td>\n",
       "      <td>40.756680</td>\n",
       "      <td>N</td>\n",
       "      <td>3795.9</td>\n",
       "      <td>424.6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id3505355</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.964203</td>\n",
       "      <td>40.679993</td>\n",
       "      <td>-73.959808</td>\n",
       "      <td>40.655403</td>\n",
       "      <td>N</td>\n",
       "      <td>2904.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1217141</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.997437</td>\n",
       "      <td>40.737583</td>\n",
       "      <td>-73.986160</td>\n",
       "      <td>40.729523</td>\n",
       "      <td>N</td>\n",
       "      <td>1499.5</td>\n",
       "      <td>193.2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id2150126</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-30 23:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.956070</td>\n",
       "      <td>40.771900</td>\n",
       "      <td>-73.986427</td>\n",
       "      <td>40.730469</td>\n",
       "      <td>N</td>\n",
       "      <td>7023.9</td>\n",
       "      <td>494.8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id1598245</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.970215</td>\n",
       "      <td>40.761475</td>\n",
       "      <td>-73.961510</td>\n",
       "      <td>40.755890</td>\n",
       "      <td>N</td>\n",
       "      <td>1108.2</td>\n",
       "      <td>103.2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime  passenger_count  \\\n",
       "0  id3004672          1 2016-06-30 23:59:00                1   \n",
       "1  id3505355          1 2016-06-30 23:59:00                1   \n",
       "2  id1217141          1 2016-06-30 23:59:00                1   \n",
       "3  id2150126          2 2016-06-30 23:59:00                1   \n",
       "4  id1598245          1 2016-06-30 23:59:00                1   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.988129        40.732029         -73.990173         40.756680   \n",
       "1        -73.964203        40.679993         -73.959808         40.655403   \n",
       "2        -73.997437        40.737583         -73.986160         40.729523   \n",
       "3        -73.956070        40.771900         -73.986427         40.730469   \n",
       "4        -73.970215        40.761475         -73.961510         40.755890   \n",
       "\n",
       "  store_and_fwd_flag  total_distance  total_travel_time  number_of_steps  \n",
       "0                  N          3795.9              424.6                4  \n",
       "1                  N          2904.5              200.0                4  \n",
       "2                  N          1499.5              193.2                4  \n",
       "3                  N          7023.9              494.8               11  \n",
       "4                  N          1108.2              103.2                4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(path+'/test.csv', parse_dates=['pickup_datetime'])\n",
    "fast_route_test = pd.read_csv(path+'/fastest_routes_test.csv')\n",
    "fast_route_test_new = fast_route_test[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\n",
    "test_df = pd.merge(df_test, fast_route_test_new, on = 'id', how = 'left')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting date from the datetime column from test and train\n",
    "train_df.loc[:, 'pickup_date'] = train_df['pickup_datetime'].dt.date\n",
    "train_df.loc[:, 'dropoff_date'] = train_df['dropoff_datetime'].dt.date\n",
    "test_df.loc[:, 'pickup_date'] = test_df['pickup_datetime'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the store_and_fwd_flag into categorical variable\n",
    "train_df['store_and_fwd_flag'] = 1 * (train_df.store_and_fwd_flag.values == 'Y')\n",
    "test_df['store_and_fwd_flag'] = 1 * (test_df.store_and_fwd_flag.values == 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>total_distance</th>\n",
       "      <th>total_travel_time</th>\n",
       "      <th>number_of_steps</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>dropoff_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>0</td>\n",
       "      <td>455</td>\n",
       "      <td>2009.1</td>\n",
       "      <td>164.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>2016-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>0</td>\n",
       "      <td>663</td>\n",
       "      <td>2513.2</td>\n",
       "      <td>332.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>2016-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>0</td>\n",
       "      <td>2124</td>\n",
       "      <td>11060.8</td>\n",
       "      <td>767.6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>2016-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>0</td>\n",
       "      <td>429</td>\n",
       "      <td>1779.4</td>\n",
       "      <td>235.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>2016-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "      <td>1614.9</td>\n",
       "      <td>140.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>2016-03-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime    dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55 2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35 2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24 2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31 2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55 2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude  store_and_fwd_flag  trip_duration  total_distance  \\\n",
       "0         40.765602                   0            455          2009.1   \n",
       "1         40.731152                   0            663          2513.2   \n",
       "2         40.710087                   0           2124         11060.8   \n",
       "3         40.706718                   0            429          1779.4   \n",
       "4         40.782520                   0            435          1614.9   \n",
       "\n",
       "   total_travel_time  number_of_steps pickup_date dropoff_date  \n",
       "0              164.9              5.0  2016-03-14   2016-03-14  \n",
       "1              332.0              6.0  2016-06-12   2016-06-12  \n",
       "2              767.6             16.0  2016-01-19   2016-01-19  \n",
       "3              235.8              4.0  2016-04-06   2016-04-06  \n",
       "4              140.1              5.0  2016-03-26   2016-03-26  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Feature Extraction</b></h3>\n",
    "\n",
    "In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.\n",
    "\n",
    "When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection.The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data\n",
    "\n",
    "<h3>Principal Component Analysis</h3>\n",
    "\n",
    "Sometimes data are collected on a large number of variables from a single population. With a large number of variables, the dispersion matrix may be too large to study and interpret properly. There would be too many pairwise correlations between the variables to consider. Graphical displays may also not be particularly helpful when the data set is very large. With 12 variables, for example, there will be more than 200 three-dimensional scatterplots. To interpret the data in a more meaningful form, it is necessary to reduce the number of variables to a few, interpretable linear combinations of the data. Each linear combination will correspond to a principal component.<b>We use PCA to transform longitude and latitude coordinates.\n",
    "In this case it is not about dimension reduction since we transform 2D-> 2D. The rotation could help for decision tree splits.</b>. With rotation the principal components are more specifically associated with the original variables. The maximum likelihood solution is undefined down to an orthogonal transformation (ie rotations) of the factor loadings. That is, any rotation of the factor loadings is an equally good solutions. So it makes sense to rotate these loadings in a way which maximises ease of interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack arrays in sequence vertically (row wise).\n",
    "coords = np.vstack((train_df[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    train_df[['dropoff_latitude', 'dropoff_longitude']].values,\n",
    "                    train_df[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    train_df[['dropoff_latitude', 'dropoff_longitude']].values))\n",
    "\n",
    "pca = PCA().fit(coords)\n",
    "\n",
    "train_df['pickup_pca0'] = pca.transform(train_df[['pickup_latitude', 'pickup_longitude']])[:,0]\n",
    "train_df['pickup_pca1'] = pca.transform(train_df[['pickup_latitude', 'pickup_longitude']])[:,1]\n",
    "train_df['dropoff_pca0'] = pca.transform(train_df[['dropoff_latitude', 'dropoff_longitude']])[:,0]\n",
    "train_df['dropoff_pca1'] = pca.transform(train_df[['dropoff_latitude', 'dropoff_longitude']])[:,1]\n",
    "\n",
    "test_df['pickup_pca0'] = pca.transform(test_df[['pickup_latitude', 'pickup_longitude']])[:,0]\n",
    "test_df['pickup_pca1'] = pca.transform(test_df[['pickup_latitude', 'pickup_longitude']])[:,1]\n",
    "test_df['dropoff_pca0'] = pca.transform(test_df[['dropoff_latitude', 'dropoff_longitude']])[:,0]\n",
    "test_df['dropoff_pca1'] = pca.transform(test_df[['dropoff_latitude', 'dropoff_longitude']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>total_distance</th>\n",
       "      <th>total_travel_time</th>\n",
       "      <th>number_of_steps</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>dropoff_date</th>\n",
       "      <th>pickup_pca0</th>\n",
       "      <th>pickup_pca1</th>\n",
       "      <th>dropoff_pca0</th>\n",
       "      <th>dropoff_pca1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>0</td>\n",
       "      <td>455</td>\n",
       "      <td>2009.1</td>\n",
       "      <td>164.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>-0.009496</td>\n",
       "      <td>0.013801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>0</td>\n",
       "      <td>663</td>\n",
       "      <td>2513.2</td>\n",
       "      <td>332.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>-0.012447</td>\n",
       "      <td>0.026972</td>\n",
       "      <td>-0.018933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>0</td>\n",
       "      <td>2124</td>\n",
       "      <td>11060.8</td>\n",
       "      <td>767.6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.033831</td>\n",
       "      <td>-0.039692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>0</td>\n",
       "      <td>429</td>\n",
       "      <td>1779.4</td>\n",
       "      <td>235.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>0.038057</td>\n",
       "      <td>-0.029593</td>\n",
       "      <td>0.040920</td>\n",
       "      <td>-0.042722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "      <td>1614.9</td>\n",
       "      <td>140.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>-0.002411</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>-0.002026</td>\n",
       "      <td>0.031099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime    dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55 2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35 2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24 2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31 2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55 2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude  store_and_fwd_flag  trip_duration  total_distance  \\\n",
       "0         40.765602                   0            455          2009.1   \n",
       "1         40.731152                   0            663          2513.2   \n",
       "2         40.710087                   0           2124         11060.8   \n",
       "3         40.706718                   0            429          1779.4   \n",
       "4         40.782520                   0            435          1614.9   \n",
       "\n",
       "   total_travel_time  number_of_steps pickup_date dropoff_date  pickup_pca0  \\\n",
       "0              164.9              5.0  2016-03-14   2016-03-14     0.007896   \n",
       "1              332.0              6.0  2016-06-12   2016-06-12     0.007572   \n",
       "2              767.6             16.0  2016-01-19   2016-01-19     0.004964   \n",
       "3              235.8              4.0  2016-04-06   2016-04-06     0.038057   \n",
       "4              140.1              5.0  2016-03-26   2016-03-26    -0.002411   \n",
       "\n",
       "   pickup_pca1  dropoff_pca0  dropoff_pca1  \n",
       "0     0.016976     -0.009496      0.013801  \n",
       "1    -0.012447      0.026972     -0.018933  \n",
       "2     0.012832      0.033831     -0.039692  \n",
       "3    -0.029593      0.040920     -0.042722  \n",
       "4     0.041781     -0.002026      0.031099  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Haversine Distance:</b> The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes.\n",
    "\n",
    "<b>Manhattan Distance:</b> The distance between two points in a grid based on a strictly horizontal and/or vertical path (that is, along the grid lines), as opposed to the diagonal or \"as the crow flies\" distance.\n",
    "\n",
    "<b>Bearing Distance:</b> The bearing (or azimuth) is the compass direction to travel from the starting point, and must be within the range 0 to 360. 0 represents north, 90 is east, 180 is south and 270 is west."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) #applies a function to all the items in an input_list\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    a = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) #applies a function to all the items in an input_list\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the above three functions to the columns in train and test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'distance_haversine'] = haversine_array(train_df['pickup_latitude'].values, train_df['pickup_longitude'].values, train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "train_df.loc[:, 'distance_manhattan'] = dummy_manhattan_distance(train_df['pickup_latitude'].values, train_df['pickup_longitude'].values, train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "train_df.loc[:, 'direction'] = bearing_array(train_df['pickup_latitude'].values, train_df['pickup_longitude'].values, train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "train_df.loc[:, 'pca_manhattan'] = np.abs(train_df['dropoff_pca1'] - train_df['pickup_pca1']) + np.abs(train_df['dropoff_pca0'] - train_df['pickup_pca0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[:, 'distance_haversine'] = haversine_array(test_df['pickup_latitude'].values, test_df['pickup_longitude'].values, test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "test_df.loc[:, 'distance_manhattan'] = dummy_manhattan_distance(test_df['pickup_latitude'].values, test_df['pickup_longitude'].values, test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "test_df.loc[:, 'direction'] = bearing_array(test_df['pickup_latitude'].values, test_df['pickup_longitude'].values, test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "test_df.loc[:, 'pca_manhattan'] = np.abs(test_df['dropoff_pca1'] - test_df['pickup_pca1']) + np.abs(test_df['dropoff_pca0'] - test_df['pickup_pca0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'center_latitude'] = (train_df['pickup_latitude'].values + train_df['dropoff_latitude'].values)/2\n",
    "train_df.loc[:, 'center_longitude'] = (train_df['pickup_longitude'].values + train_df['dropoff_longitude'].values)/2\n",
    "test_df.loc[:, 'center_latitude'] = (test_df['pickup_latitude'].values + test_df['dropoff_latitude'].values)/2\n",
    "test_df.loc[:, 'center_longitude'] = (test_df['pickup_longitude'].values + test_df['dropoff_longitude'].values)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Extract all the datetime features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'pickup_weekday'] = train_df['pickup_datetime'].dt.weekday\n",
    "train_df.loc[:, 'pickup_hour_weekofyear'] = train_df['pickup_datetime'].dt.weekofyear\n",
    "train_df.loc[:, 'pickup_hour'] = train_df['pickup_datetime'].dt.hour\n",
    "train_df.loc[:, 'pickup_minute'] = train_df['pickup_datetime'].dt.minute\n",
    "train_df.loc[:, 'pickup_dt'] = (train_df['pickup_datetime'] - train_df['pickup_datetime'].min()).dt.total_seconds()\n",
    "train_df.loc[:, 'pickup_week_hour'] = train_df['pickup_weekday'] * 24 + train_df['pickup_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[:, 'pickup_weekday'] = test_df['pickup_datetime'].dt.weekday\n",
    "test_df.loc[:, 'pickup_hour_weekofyear'] = test_df['pickup_datetime'].dt.weekofyear\n",
    "test_df.loc[:, 'pickup_hour'] = test_df['pickup_datetime'].dt.hour\n",
    "test_df.loc[:, 'pickup_minute'] = test_df['pickup_datetime'].dt.minute\n",
    "test_df.loc[:, 'pickup_dt'] = (test_df['pickup_datetime'] - test_df['pickup_datetime'].min()).dt.total_seconds()\n",
    "test_df.loc[:, 'pickup_week_hour'] = test_df['pickup_weekday'] * 24 + test_df['pickup_hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract the Speed feature using the Haversine Distance and trip duration</h3>\n",
    "\n",
    "We will use the basic speed is equal to distance upon time formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'avg_speed_h'] = 1000 * train_df['distance_haversine']/train_df['trip_duration']\n",
    "train_df.loc[:, 'avg_speed_m'] = 1000 * train_df['distance_manhattan']/train_df['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[:, 'avg_speed_h'] = 1000 * train_df['distance_haversine']/train_df['trip_duration']\n",
    "test_df.loc[:, 'avg_speed_m'] = 1000 * train_df['distance_manhattan']/train_df['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>center_latitude</th>\n",
       "      <th>center_longitude</th>\n",
       "      <th>pickup_weekday</th>\n",
       "      <th>pickup_hour_weekofyear</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_minute</th>\n",
       "      <th>pickup_dt</th>\n",
       "      <th>pickup_week_hour</th>\n",
       "      <th>avg_speed_h</th>\n",
       "      <th>avg_speed_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.766769</td>\n",
       "      <td>-73.973392</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>6369878.0</td>\n",
       "      <td>17</td>\n",
       "      <td>3.293452</td>\n",
       "      <td>3.814139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.734858</td>\n",
       "      <td>-73.989948</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>14085798.0</td>\n",
       "      <td>144</td>\n",
       "      <td>2.723239</td>\n",
       "      <td>3.665922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.737013</td>\n",
       "      <td>-73.992180</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>1596907.0</td>\n",
       "      <td>35</td>\n",
       "      <td>3.006167</td>\n",
       "      <td>3.862323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.713345</td>\n",
       "      <td>-74.011154</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>8364734.0</td>\n",
       "      <td>67</td>\n",
       "      <td>3.462700</td>\n",
       "      <td>3.872567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.787865</td>\n",
       "      <td>-73.972988</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>7392638.0</td>\n",
       "      <td>133</td>\n",
       "      <td>2.732387</td>\n",
       "      <td>2.757372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime    dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55 2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35 2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24 2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31 2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55 2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude  store_and_fwd_flag     ...       center_latitude  \\\n",
       "0         40.765602                   0     ...             40.766769   \n",
       "1         40.731152                   0     ...             40.734858   \n",
       "2         40.710087                   0     ...             40.737013   \n",
       "3         40.706718                   0     ...             40.713345   \n",
       "4         40.782520                   0     ...             40.787865   \n",
       "\n",
       "   center_longitude  pickup_weekday  pickup_hour_weekofyear pickup_hour  \\\n",
       "0        -73.973392               0                      11          17   \n",
       "1        -73.989948               6                      23           0   \n",
       "2        -73.992180               1                       3          11   \n",
       "3        -74.011154               2                      14          19   \n",
       "4        -73.972988               5                      12          13   \n",
       "\n",
       "  pickup_minute   pickup_dt  pickup_week_hour  avg_speed_h  avg_speed_m  \n",
       "0            24   6369878.0                17     3.293452     3.814139  \n",
       "1            43  14085798.0               144     2.723239     3.665922  \n",
       "2            35   1596907.0                35     3.006167     3.862323  \n",
       "3            32   8364734.0                67     3.462700     3.872567  \n",
       "4            30   7392638.0               133     2.732387     2.757372  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adding holiday as a feature</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add all the US holidays as holiday feature from the available library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "from datetime import date, datetime\n",
    "us_holidays = holidays.UnitedStates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If that particular day is a holiday then it will be marked as 1 otherwise 0\n",
    "train_df.loc[:, 'holiday'] = train_df.pickup_datetime.apply(lambda x: 1 if x in us_holidays else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for the test data\n",
    "test_df.loc[:, 'holiday'] = train_df.pickup_datetime.apply(lambda x: 1 if x in us_holidays else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adding weather as a feature</h3>\n",
    "\n",
    "First we will import the weather data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>maximum temperature</th>\n",
       "      <th>minimum temperature</th>\n",
       "      <th>average temperature</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>snow fall</th>\n",
       "      <th>snow depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-1-2016</td>\n",
       "      <td>42</td>\n",
       "      <td>34</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-1-2016</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3-1-2016</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4-1-2016</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5-1-2016</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  maximum temperature  minimum temperature  average temperature  \\\n",
       "0  1-1-2016                   42                   34                 38.0   \n",
       "1  2-1-2016                   40                   32                 36.0   \n",
       "2  3-1-2016                   45                   35                 40.0   \n",
       "3  4-1-2016                   36                   14                 25.0   \n",
       "4  5-1-2016                   29                   11                 20.0   \n",
       "\n",
       "  precipitation snow fall snow depth  \n",
       "0          0.00       0.0          0  \n",
       "1          0.00       0.0          0  \n",
       "2          0.00       0.0          0  \n",
       "3          0.00       0.0          0  \n",
       "4          0.00       0.0          0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()+'\\weather_data_nyc_centralpark_2016.csv'\n",
    "weather_df = pd.read_csv(path)\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['DATE'] = pd.to_datetime(weather_df.date) #as the format of date in data varies we will convert it into one format\n",
    "weather_df['pickup_date'] = weather_df['DATE'].dt.date #rename the column name\n",
    "del weather_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will merge the weather data with the train and test data\n",
    "train_df = pd.merge(train_df, weather_df, on = 'pickup_date', how = 'left')\n",
    "test_df = pd.merge(test_df, weather_df, on = 'pickup_date', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  snow fall  trip_duration\n",
      "0         0     931.256312\n",
      "1       0.0     973.658589\n",
      "2       0.2     934.412834\n",
      "3       0.4     906.369139\n",
      "4       0.5     828.804774\n",
      "5       1.4     813.655093\n",
      "6       2.5     875.932462\n",
      "7      27.3    1026.549757\n",
      "8         T     899.071886\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x201c2dfd898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We will try and visualize how weather affected the trip duration\n",
    "a = train_df.groupby(['snow fall']).mean()['trip_duration'].reset_index()\n",
    "print(a)\n",
    "sns.barplot(x=\"snow fall\", y='trip_duration', data=a)\n",
    "plt.xlabel('snow fall')\n",
    "plt.ylabel('trip_duration')\n",
    "plt.title('Snow Fall v.s. Avg Trip Duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>From the above graph it is clearly visible that during snowfall the drip duration of the taxi ride increased</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create a lookup for the 'T' value in snowfall column so as to convert it into float 0.01\n",
    "lookup = {\"snow fall\": {\"T\": 0.01},\n",
    "          \"snow depth\": {\"T\": 0.01},\n",
    "          'precipitation': {\"T\": 0.01}}\n",
    "train_df.replace(lookup, inplace=True)\n",
    "train_df['snow fall'] = train_df['snow fall'].astype('float')\n",
    "train_df['snow depth'] = train_df['snow depth'].astype('float')\n",
    "train_df['precipitation'] = train_df['precipitation'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will do the same thing for test data\n",
    "test_df.replace(lookup, inplace=True)\n",
    "test_df['snow fall'] = test_df['snow fall'].astype('float')\n",
    "test_df['snow depth'] = test_df['snow depth'].astype('float')\n",
    "test_df['precipitation'] = test_df['precipitation'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                object\n",
       "vendor_id                          int64\n",
       "pickup_datetime           datetime64[ns]\n",
       "dropoff_datetime          datetime64[ns]\n",
       "passenger_count                    int64\n",
       "pickup_longitude                 float64\n",
       "pickup_latitude                  float64\n",
       "dropoff_longitude                float64\n",
       "dropoff_latitude                 float64\n",
       "store_and_fwd_flag                 int32\n",
       "trip_duration                      int64\n",
       "total_distance                   float64\n",
       "total_travel_time                float64\n",
       "number_of_steps                  float64\n",
       "pickup_date                       object\n",
       "dropoff_date                      object\n",
       "pickup_pca0                      float64\n",
       "pickup_pca1                      float64\n",
       "dropoff_pca0                     float64\n",
       "dropoff_pca1                     float64\n",
       "distance_haversine               float64\n",
       "distance_manhattan               float64\n",
       "direction                        float64\n",
       "pca_manhattan                    float64\n",
       "center_latitude                  float64\n",
       "center_longitude                 float64\n",
       "pickup_weekday                     int64\n",
       "pickup_hour_weekofyear             int64\n",
       "pickup_hour                        int64\n",
       "pickup_minute                      int64\n",
       "pickup_dt                        float64\n",
       "pickup_week_hour                   int64\n",
       "avg_speed_h                      float64\n",
       "avg_speed_m                      float64\n",
       "holiday                            int64\n",
       "maximum temperature                int64\n",
       "minimum temperature                int64\n",
       "average temperature              float64\n",
       "precipitation                    float64\n",
       "snow fall                        float64\n",
       "snow depth                       float64\n",
       "DATE                      datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After adding all the extra features, we will check for the datatype of all the columns in train and test\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        0\n",
       "vendor_id                 0\n",
       "pickup_datetime           0\n",
       "dropoff_datetime          0\n",
       "passenger_count           0\n",
       "pickup_longitude          0\n",
       "pickup_latitude           0\n",
       "dropoff_longitude         0\n",
       "dropoff_latitude          0\n",
       "store_and_fwd_flag        0\n",
       "trip_duration             0\n",
       "total_distance            1\n",
       "total_travel_time         1\n",
       "number_of_steps           1\n",
       "pickup_date               0\n",
       "dropoff_date              0\n",
       "pickup_pca0               0\n",
       "pickup_pca1               0\n",
       "dropoff_pca0              0\n",
       "dropoff_pca1              0\n",
       "distance_haversine        0\n",
       "distance_manhattan        0\n",
       "direction                 0\n",
       "pca_manhattan             0\n",
       "center_latitude           0\n",
       "center_longitude          0\n",
       "pickup_weekday            0\n",
       "pickup_hour_weekofyear    0\n",
       "pickup_hour               0\n",
       "pickup_minute             0\n",
       "pickup_dt                 0\n",
       "pickup_week_hour          0\n",
       "avg_speed_h               0\n",
       "avg_speed_m               0\n",
       "holiday                   0\n",
       "maximum temperature       0\n",
       "minimum temperature       0\n",
       "average temperature       0\n",
       "precipitation             0\n",
       "snow fall                 0\n",
       "snow depth                0\n",
       "DATE                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will also check for null values in train data\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 3 columns having null values will be replaced with the mean value of that particular column\n",
    "train_df['total_distance'].fillna((train_df['total_distance'].mean()), inplace=True)\n",
    "train_df['total_travel_time'].fillna((train_df['total_travel_time'].mean()), inplace=True)\n",
    "train_df['number_of_steps'].fillna((train_df['number_of_steps'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        0\n",
       "vendor_id                 0\n",
       "pickup_datetime           0\n",
       "passenger_count           0\n",
       "pickup_longitude          0\n",
       "pickup_latitude           0\n",
       "dropoff_longitude         0\n",
       "dropoff_latitude          0\n",
       "store_and_fwd_flag        0\n",
       "total_distance            0\n",
       "total_travel_time         0\n",
       "number_of_steps           0\n",
       "pickup_date               0\n",
       "pickup_pca0               0\n",
       "pickup_pca1               0\n",
       "dropoff_pca0              0\n",
       "dropoff_pca1              0\n",
       "distance_haversine        0\n",
       "distance_manhattan        0\n",
       "direction                 0\n",
       "pca_manhattan             0\n",
       "center_latitude           0\n",
       "center_longitude          0\n",
       "pickup_weekday            0\n",
       "pickup_hour_weekofyear    0\n",
       "pickup_hour               0\n",
       "pickup_minute             0\n",
       "pickup_dt                 0\n",
       "pickup_week_hour          0\n",
       "avg_speed_h               0\n",
       "avg_speed_m               0\n",
       "holiday                   0\n",
       "maximum temperature       0\n",
       "minimum temperature       0\n",
       "average temperature       0\n",
       "precipitation             0\n",
       "snow fall                 0\n",
       "snow depth                0\n",
       "DATE                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the names of all the columns in train into a list and the list of not required columns to exclude list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendor_id                 1458644\n",
       "passenger_count           1458644\n",
       "pickup_longitude          1458644\n",
       "pickup_latitude           1458644\n",
       "dropoff_longitude         1458644\n",
       "dropoff_latitude          1458644\n",
       "store_and_fwd_flag        1458644\n",
       "total_distance            1458644\n",
       "total_travel_time         1458644\n",
       "number_of_steps           1458644\n",
       "pickup_pca0               1458644\n",
       "pickup_pca1               1458644\n",
       "dropoff_pca0              1458644\n",
       "dropoff_pca1              1458644\n",
       "distance_haversine        1458644\n",
       "distance_manhattan        1458644\n",
       "direction                 1458644\n",
       "pca_manhattan             1458644\n",
       "center_latitude           1458644\n",
       "center_longitude          1458644\n",
       "pickup_weekday            1458644\n",
       "pickup_hour_weekofyear    1458644\n",
       "pickup_hour               1458644\n",
       "pickup_minute             1458644\n",
       "pickup_dt                 1458644\n",
       "pickup_week_hour          1458644\n",
       "avg_speed_h               1458644\n",
       "avg_speed_m               1458644\n",
       "holiday                   1458644\n",
       "maximum temperature       1458644\n",
       "minimum temperature       1458644\n",
       "average temperature       1458644\n",
       "precipitation             1458644\n",
       "snow fall                 1458644\n",
       "snow depth                1458644\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = list(train_df.columns)\n",
    "exclude = ['id','pickup_datetime','dropoff_datetime','trip_duration','pickup_date','dropoff_date','DATE']\n",
    "feature_names = [f for f in train_df.columns if f not in exclude]\n",
    "train_df[feature_names].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vendor_id', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag', 'total_distance', 'total_travel_time', 'number_of_steps', 'pickup_pca0', 'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1', 'distance_haversine', 'distance_manhattan', 'direction', 'pca_manhattan', 'center_latitude', 'center_longitude', 'pickup_weekday', 'pickup_hour_weekofyear', 'pickup_hour', 'pickup_minute', 'pickup_dt', 'pickup_week_hour', 'avg_speed_h', 'avg_speed_m', 'holiday', 'maximum temperature', 'minimum temperature', 'average temperature', 'precipitation', 'snow fall', 'snow depth']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendor_id                   int64\n",
       "passenger_count             int64\n",
       "pickup_longitude          float64\n",
       "pickup_latitude           float64\n",
       "dropoff_longitude         float64\n",
       "dropoff_latitude          float64\n",
       "store_and_fwd_flag          int32\n",
       "total_distance            float64\n",
       "total_travel_time         float64\n",
       "number_of_steps           float64\n",
       "pickup_pca0               float64\n",
       "pickup_pca1               float64\n",
       "dropoff_pca0              float64\n",
       "dropoff_pca1              float64\n",
       "distance_haversine        float64\n",
       "distance_manhattan        float64\n",
       "direction                 float64\n",
       "pca_manhattan             float64\n",
       "center_latitude           float64\n",
       "center_longitude          float64\n",
       "pickup_weekday              int64\n",
       "pickup_hour_weekofyear      int64\n",
       "pickup_hour                 int64\n",
       "pickup_minute               int64\n",
       "pickup_dt                 float64\n",
       "pickup_week_hour            int64\n",
       "avg_speed_h               float64\n",
       "avg_speed_m               float64\n",
       "holiday                     int64\n",
       "maximum temperature         int64\n",
       "minimum temperature         int64\n",
       "average temperature       float64\n",
       "precipitation             float64\n",
       "snow fall                 float64\n",
       "snow depth                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[feature_names].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************************************\n",
    "#*    This was adapted from a post from\n",
    "#*    Author: Kulbear\n",
    "#*    Date: 04/23/2018\n",
    "#*    Availability: https://github.com/Kulbear/New-York-City-Taxi-Trip-Duration/blob/master/xgboost.ipynb\n",
    "#*\n",
    "#***************************************************************************************/\n",
    "\n",
    "import xgboost as xgb\n",
    "y = np.log(train_df['trip_duration'].values + 1) #We will convert the trip duration into logarithmic values for the sake of ML\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_df[feature_names].values, y, test_size=0.2) # split the dataset into train and test\n",
    "\n",
    "#The data is stored in a DMatrix object\n",
    "#Missing values can be replaced by a default value in the DMatrix constructor\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(val_x, label=val_y )\n",
    "dtest = xgb.DMatrix(test_df[feature_names].values)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "# We have used random search for XGBoost\n",
    "xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n",
    "            'subsample': 0.8, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'eval_metric': 'rmse', 'objective': 'reg:linear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:4.21862\tvalid-rmse:4.21747\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 50 rounds.\n",
      "[10]\ttrain-rmse:0.176757\tvalid-rmse:0.181808\n",
      "[20]\ttrain-rmse:0.108635\tvalid-rmse:0.118087\n",
      "[30]\ttrain-rmse:0.10046\tvalid-rmse:0.112307\n",
      "[40]\ttrain-rmse:0.095502\tvalid-rmse:0.110124\n",
      "[50]\ttrain-rmse:0.0919\tvalid-rmse:0.10861\n",
      "[60]\ttrain-rmse:0.088545\tvalid-rmse:0.107903\n",
      "[70]\ttrain-rmse:0.086194\tvalid-rmse:0.107953\n",
      "[80]\ttrain-rmse:0.083958\tvalid-rmse:0.107904\n",
      "[90]\ttrain-rmse:0.080878\tvalid-rmse:0.108227\n",
      "[99]\ttrain-rmse:0.07929\tvalid-rmse:0.108135\n"
     ]
    }
   ],
   "source": [
    "# You could try to train with more epoch,the rmse value might stall after few epochs sometimes\n",
    "model = xgb.train(xgb_pars, dtrain, 100, watchlist, early_stopping_rounds=50,\n",
    "                  maximize=False, verbose_eval=10)\n",
    "\n",
    "#If you have a validation set, you can use early stopping to find the optimal number of boosting rounds. \n",
    "#Early stopping requires at least one set in evals. If there’s more than one, it will use the last.\n",
    "#train(..., evals=evals, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling RMSLE 0.10775\n"
     ]
    }
   ],
   "source": [
    "print('Modeling RMSLE %.5f' % model.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(dvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 399.8948   222.12178  916.3345  1128.9053   241.03188 3114.3386\n",
      " 2042.732    916.4408  1773.5358   429.2949 ]\n"
     ]
    }
   ],
   "source": [
    "#Predicted values\n",
    "y_pred = np.exp(predictions)-1\n",
    "print(y_pred[:10])\n",
    "y_pred = pd.DataFrame(y_pred, columns = ['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 393.  216.  940. 1136.  239. 3140. 2227.  990. 1683.  434.]\n"
     ]
    }
   ],
   "source": [
    "#Original values\n",
    "y_org = np.exp(val_y)-1\n",
    "print(y_org[:10])\n",
    "y_org= pd.DataFrame(y_org, columns = ['y_org'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_org</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>393.0</td>\n",
       "      <td>399.894806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216.0</td>\n",
       "      <td>222.121780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>940.0</td>\n",
       "      <td>916.334473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1136.0</td>\n",
       "      <td>1128.905273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>239.0</td>\n",
       "      <td>241.031876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_org       y_pred\n",
       "0   393.0   399.894806\n",
       "1   216.0   222.121780\n",
       "2   940.0   916.334473\n",
       "3  1136.0  1128.905273\n",
       "4   239.0   241.031876"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comparing the predicted and original values\n",
    "result = pd.concat([y_org,y_pred], axis=1)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Regressor</h2>\n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.Random decision forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)\n",
    "\n",
    "Randomized search on hyper parameters.\n",
    "\n",
    "RandomizedSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.\n",
    "\n",
    "In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\n",
    "\n",
    "If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9826610120711197\n",
      "10\n",
      "20\n",
      "5\n",
      "1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf = RandomForestRegressor(random_state =42)#random_state is the seed used by the random number generator;\n",
    "\n",
    "#We will try and select the best parameters for Random Forest to give good accuracy\n",
    "param_grid = {\"max_depth\": [10,20,30],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11)}\n",
    "\n",
    "validator = RandomizedSearchCV(clf, param_distributions= param_grid) #clf is the estimator and\n",
    "validator.fit(train_x,train_y)\n",
    "#param_distributions : dict\n",
    "#Dictionary with parameters names (string) as keys and distributions or lists of parameters to try\n",
    "\n",
    "print(validator.best_score_)\n",
    "print(validator.best_estimator_.n_estimators)\n",
    "print(validator.best_estimator_.max_depth)\n",
    "print(validator.best_estimator_.min_samples_split)\n",
    "print(validator.best_estimator_.min_samples_leaf)\n",
    "print(validator.best_estimator_.max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>RandomForestRegressor</h6>\n",
    "\n",
    "<h6>max_depth</h6> : integer or None, optional (default=None)\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "<h6>min_samples_split</h6>int, float, optional (default=2)\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "<h6>min_samples_leaf</h6>int, float, optional (default=1)\n",
    "The minimum number of samples required to be at a leaf node:\n",
    "\n",
    "<h6>n_estimators</h6>: The number of trees in the forest.\n",
    "\n",
    "<h6>max_features</h6>: The number of features to consider when looking for the best split\n",
    "\n",
    "You can vary the features and see it the rmse value decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
       "           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=5,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(max_depth = 20 , min_samples_split= 2, \n",
    "                                 min_samples_leaf=5, n_estimators =10, max_features =8)\n",
    "rf_model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on validation set\n",
    "predictions = rf_model.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11033131704917128\n"
     ]
    }
   ],
   "source": [
    "#RMSLE error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#tree_pred = pd.DataFrame(predictions)\n",
    "dec_mse = mean_squared_error(predictions, val_y)\n",
    "rmse = np.sqrt(dec_mse)\n",
    "print(rmse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lasso & Ridge Regularization</h2>\n",
    "\n",
    "Ridge and Lasso regression are powerful techniques generally used for creating parsimonious models in presence of a ‘large’ number of features. Here ‘large’ can typically mean either of two things:\n",
    "    1.Large enough to enhance the tendency of a model to overfit (as low as 10 variables might cause overfitting)\n",
    "    2.Large enough to cause computational challenges. With modern systems, this situation might arise in case of millions or   billions of features\n",
    "  \n",
    "Though Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. If you’ve heard of them before, you must know that they work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques. The key difference is in how they assign penalty to the coefficients:\n",
    "\n",
    "Ridge Regression:\n",
    "Performs L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
    "\n",
    "Lasso Regression:\n",
    "Performs L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of absolute value of coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>param_grid :</h6> dict of string to sequence, or sequence of such\n",
    "\n",
    "The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.\n",
    "\n",
    "An empty dict signifies default parameters.\n",
    "\n",
    "A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ridge</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49843961136619813\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "#Ridge\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#This model solves a regression model \n",
    "#where the loss function is the linear least squares function and regularization is given by the l2-norm\n",
    "ridge = Ridge()\n",
    "\n",
    "#param_grid will be give a range of values for alpha, which provides regularization strength in Ridge function\n",
    "param_grid = {\"alpha\": [0.01,0.05,0.001,0.0001,0.000001,0.005,0.0005,0.00005]}\n",
    "\n",
    "validator = GridSearchCV(ridge, param_grid= param_grid) \n",
    "validator.fit(train_x,train_y)\n",
    "print(validator.best_score_)\n",
    "print(validator.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='cholesky', tol=0.001)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.\n",
    "ridge_reg = Ridge(alpha = 0.05, solver = 'cholesky')\n",
    "ridge_reg.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge = ridge_reg.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5108069344777587\n"
     ]
    }
   ],
   "source": [
    "dec_mse_ridge = mean_squared_error(predictions_ridge, val_y)\n",
    "rmse_ridge = np.sqrt(dec_mse_ridge)\n",
    "print(rmse_ridge) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lasso</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5017025114087248\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "param_grid = {\"alpha\": [0.01,0.05,0.001,0.0001,0.000001,0.005,0.0005,0.00005]}\n",
    "\n",
    "validator = GridSearchCV(lasso, param_grid= param_grid) \n",
    "validator.fit(train_x,train_y)\n",
    "print(validator.best_score_)\n",
    "print(validator.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.05, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=1,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alpha: Constant that multiplies the L1 term\n",
    "#random_state: The seed of the pseudo random number generator that selects a random feature to update\n",
    "#The parameters can be changed\n",
    "lasso_reg=Lasso(alpha =0.05, random_state=1)\n",
    "lasso_reg.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lasso = lasso_reg.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.529727936744906\n"
     ]
    }
   ],
   "source": [
    "dec_mse_lasso = mean_squared_error(predictions_lasso,val_y)\n",
    "rmse_lasso = np.sqrt(dec_mse_lasso)\n",
    "print(rmse_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ensemble Learning</h2>\n",
    "\n",
    "An ensemble contains a number of learners which are usually called base learners. The generalization ability of an ensemble\n",
    "is usually much stronger than that of base learners. Actually, ensemble learning is appealing because that it is able to boost\n",
    "weak learners which are slightly better than random guess to strong learners which can make very accurate predictions. So,\n",
    "“base learners” are also referred as “weak learners”. It is noteworthy, however, that although most theoretical analyses work\n",
    "on weak learners, base learners used in practice are not necessarily weak since using not-so-weak base learners often results\n",
    "in better performance.\n",
    "\n",
    "Base learners are usually generated from training data by a base learning algorithm which can be decision tree, neural\n",
    "network or other kinds of machine learning algorithms. Most ensemble methods use a single base learning algorithm to\n",
    "produce homogeneous base learners, but there are also some methods which use multiple learning algorithms to produce\n",
    "heterogeneous learners. In the latter case there is no single base learning algorithm and thus, some people prefer calling the\n",
    "learners individual learners or component learners to “base learners”, while the names “individual learners” and “component\n",
    "learners” can also be used for homogeneous base learners.\n",
    "\n",
    "To see if we get better performance we will combine Random Forest, Lasso, Ridge and XGBRegressor and check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>XGBRegressor</h6>\n",
    "\n",
    "objective : string or callable\n",
    "Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below)\n",
    "\n",
    "n_estimators : int\n",
    "Number of boosted trees to fit\n",
    "\n",
    "subsample : float\n",
    "Subsample ratio of the training instance.\n",
    "\n",
    "max_depth : int\n",
    "Maximum tree depth for base learners\n",
    "\n",
    "min_child_weight : int\n",
    "Minimum sum of instance weight(hessian) needed in a child.\n",
    "\n",
    "reg_alpha : float (xgb’s alpha)\n",
    "L1 regularization term on weights\n",
    "reg_lambda : float (xgb’s lambda)\n",
    "L2 regularization term on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor(objective='reg:linear', n_estimators=10, subsample=0.9, max_depth = 6, min_child_weight = 3\n",
    "                        ,reg_lambda = 2, learning_rate = 0.05)\n",
    "rf_model = RandomForestRegressor(max_depth = 20 , min_samples_split= 2, min_samples_leaf=5, n_estimators =10, max_features =8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "\n",
    "\n",
    "class AveragingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, regressors):\n",
    "        self.regressors = regressors\n",
    "        self.predictions = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for regr in self.regressors:\n",
    "            regr.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.predictions = np.column_stack([regr.predict(X) for regr in self.regressors])\n",
    "        return np.mean(self.predictions, axis=1)\n",
    "    \n",
    "    \n",
    "averaged_model = AveragingRegressor([xgb_model, rf_model,lasso_reg,ridge_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_model.fit(train_x,train_y)\n",
    "predicitons_ensemble = averaged_model.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9652056875334333\n"
     ]
    }
   ],
   "source": [
    "dec_mse_ensemble = mean_squared_error(predicitons_ensemble, val_y)\n",
    "rmse_ensemble = np.sqrt(dec_mse_ensemble)\n",
    "print(rmse_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>\n",
    "\n",
    "An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.\n",
    "\n",
    "I have used keras module instead of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log)))\n",
    "\n",
    "rmsle = root_mean_squared_logarithmic_error\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = train_x.shape[-1]\n",
    "_input = Input(shape=(num_features,))\n",
    "#_norm = BatchNormalization()(_input)\n",
    "layer1 = Dense(150, activation='linear')(_input)\n",
    "layer1 = Dropout(0.2)(layer1)\n",
    "layer2 = Dense(20, activation='relu')(layer1)\n",
    "_output = Dense(1, activation='relu')(layer2)\n",
    "\n",
    "model = Model(inputs=[_input], outputs=[_output])\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 933532 samples, validate on 233383 samples\n",
      "Epoch 1/50\n",
      "933532/933532 [==============================] - 14s 15us/step - loss: 2.0164 - val_loss: 2.0074\n",
      "Epoch 2/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 3/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 4/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 5/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 6/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 7/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 8/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 9/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 10/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 11/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 12/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 13/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 14/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 15/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 16/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 17/50\n",
      "933532/933532 [==============================] - 14s 15us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 18/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 19/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 20/50\n",
      "933532/933532 [==============================] - 13s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 21/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 22/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 23/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 24/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 25/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 26/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 27/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 28/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 29/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 30/50\n",
      "933532/933532 [==============================] - 13s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 31/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 32/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 33/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 34/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 35/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 36/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 37/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 38/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 39/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 40/50\n",
      "933532/933532 [==============================] - 23s 24us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 41/50\n",
      "933532/933532 [==============================] - 25s 26us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 42/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 43/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 44/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 45/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 46/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 47/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 48/50\n",
      "933532/933532 [==============================] - 12s 13us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 49/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n",
      "Epoch 50/50\n",
      "933532/933532 [==============================] - 13s 14us/step - loss: 2.0077 - val_loss: 2.0074\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights1.h5', save_best_only=True,\n",
    "                             monitor='val_loss', verbose=0)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "              patience=10, verbose=1)\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 40:\n",
    "        return learning_rate\n",
    "    if epoch > 40 and epoch < 80:\n",
    "        return learning_rate * 0.1\n",
    "    else:\n",
    "        return learning_rate * 0.05\n",
    "    \n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "results = model.fit(train_x, train_y, \n",
    "                    batch_size=256,\n",
    "                    epochs=50, callbacks=[checkpoint],\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 42.43811166912354\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model.predict(val_x)\n",
    "print('Test MSE: {}'.format(mean_squared_error(val_y, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.00744\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "for yt, yth in zip(val_y, y_test_hat):\n",
    "    diff = np.log(yt + 1.) - np.log(yth + 1.)\n",
    "    test_loss.append(diff ** 2)\n",
    "print(np.sqrt(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "\n",
    "Evaluation of models\n",
    "XGBoost: 0.1077\n",
    "RandomForest: 0.1103\n",
    "Ridge: 0.5107\n",
    "Lasso: 0.5297\n",
    "EnsembleLearning: 0.9652\n",
    "NeraulNetwork: 2.0074\n",
    "\n",
    "Among all the models XGBoost performed exceptionally well when compared to other algorithms\n",
    "It is not a rule that Ensemble Learning would give better results, sometimes it may perform worse than independent models\n",
    "\n",
    "After adding holiday, weather, fast route features to the original data, the rmse value of all the algorithms improved which means it gave good accuracy.\n",
    "\n",
    "<b>Further Improvements</b>\n",
    "The dataset doesn't contain sufficient amount of information to predict the target variable. To be able to predict and get good accuracy for prediction external data was needed to add to the original data. There very instances where the trip duration was very large for smaller distance, the reason behind them must be different like traffic jam, weather or increase number of tourist etc. The dataset gives only the pickup and drop off co-ordinates and not the route which was taken by the cab(The taxi might have used a longer route instead of a shorter one). Inclusion of all such factors will help in predicting the right trip duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>License (MIT)</h1>\n",
    "\n",
    "Copyright (c) 2018 by Ninad Gadre\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
